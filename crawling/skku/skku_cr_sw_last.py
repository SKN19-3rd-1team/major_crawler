from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import time
import json

departments = {
    "ì†Œí”„íŠ¸ì›¨ì–´í•™ê³¼": "sw",
    "ì»´í“¨í„°ê³µí•™ê³¼": "computer",
    "ê¸€ë¡œë²Œìœµí•©í•™ë¶€": "global",
    "ì§€ëŠ¥í˜•ì†Œí”„íŠ¸ì›¨ì–´í•™ê³¼": "intelli"
}

driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
driver.maximize_window()

base_curriculum_url = "https://sw.skku.edu/sw/under_{}_curriculum.do"

result_json = {
    "ì„±ê· ê´€ëŒ€í•™êµ": {
        "ì†Œí”„íŠ¸ì›¨ì–´ìœµí•©ëŒ€í•™": {}
    }
}

# ----------------------------------------------------------
#  1) SWëŒ€í•™ í˜ì´ì§€ì˜ ìƒì„¸ìŠ¤í™: ê³¼ëª©í–‰ + ì„¤ëª…í–‰(2ì¤„ êµ¬ì¡°)
#     ì´ìˆ˜í•™ë…„ì€ td:nth-child(7)
# ----------------------------------------------------------

def crawl_curriculum_page():

    subjects = []
    rows = driver.find_elements(By.CSS_SELECTOR, "table tbody tr")

    i = 0
    while i < len(rows):

        try:
            main_row = rows[i]
            title_elem = main_row.find_element(By.CSS_SELECTOR, "td:nth-child(2) a")
            name = title_elem.text.strip()

            # â­ ì´ìˆ˜í•™ë…„ (ì»¬ëŸ¼ 7)
            grade_year = main_row.find_element(By.CSS_SELECTOR, "td:nth-child(7)").text.strip()

            # ìƒì„¸ ë‚´ìš© í¼ì¹˜ê¸°
            driver.execute_script("arguments[0].click();", title_elem)
            time.sleep(0.2)

            # ë‹¤ìŒ í–‰ì´ ì„¤ëª…í–‰
            detail_row = rows[i + 1]
            desc = detail_row.find_element(By.CSS_SELECTOR, "td").text.strip()

            subjects.append({
                "grade_year": grade_year,
                "name": name,
                "description": desc
            })

            i += 2

        except:
            i += 1

    return subjects


# ----------------------------------------------------------
#  2) offset ë°©ì‹ í˜ì´ì§€ ì „ì²´ í¬ë¡¤ë§
# ----------------------------------------------------------

def crawl_department(dept_name, dept_code):

    print(f"\n===== í•™ê³¼ í¬ë¡¤ë§ ì‹œì‘: {dept_name} =====")

    curriculum_url = base_curriculum_url.format(dept_code)
    offset = 0

    previous_titles = []
    all_subjects = []

    while True:
        url = f"{curriculum_url}?pager.offset={offset}&lang=All"
        print(f"[í˜ì´ì§€ ì´ë™] offset={offset}")

        driver.get(url)
        time.sleep(1.5)

        rows = driver.find_elements(By.CSS_SELECTOR, "table tbody tr")

        if len(rows) == 0:
            print("â†’ ê³¼ëª© ì—†ìŒ: ì¢…ë£Œ")
            break

        current_titles = []
        for i in range(0, len(rows), 2):
            try:
                title_elem = rows[i].find_element(By.CSS_SELECTOR, "td:nth-child(2) a")
                current_titles.append(title_elem.text.strip())
            except:
                pass

        if current_titles == previous_titles:
            print("â†’ ì´ì „ í˜ì´ì§€ì™€ ë™ì¼í•¨: ì¢…ë£Œ")
            break

        previous_titles = current_titles

        page_subjects = crawl_curriculum_page()
        all_subjects.extend(page_subjects)

        offset += 30
        if offset > 10000:
            print("â†’ offset ë¹„ì •ìƒ: ì¢…ë£Œ")
            break

    print(f"ì´ ê³¼ëª© ìˆ˜ì§‘: {len(all_subjects)}ê°œ")
    return all_subjects


# ----------------------------------------------------------
#  3) ì „ì²´ ì‹¤í–‰
# ----------------------------------------------------------

college = result_json["ì„±ê· ê´€ëŒ€í•™êµ"]["ì†Œí”„íŠ¸ì›¨ì–´ìœµí•©ëŒ€í•™"]

for dept_name, dept_code in departments.items():
    subjects = crawl_department(dept_name, dept_code)
    college[dept_name] = subjects

with open("skku_sw_courses_last.json", "w", encoding="utf-8") as f:
    json.dump(result_json, f, ensure_ascii=False, indent=4)

print("\nğŸ‰ ì „ì²´ í•™ê³¼ í¬ë¡¤ë§ ì™„ë£Œ!")
driver.quit()
